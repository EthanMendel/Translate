{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Within our project proposal we also linked sentence pairs from Wikipedia and GVP Corpus, but ofter reviewing the\n",
    "pairs from both of these sites, we found many inconsistencies between the translations regarding numbers, punctuation\n",
    "and structure. For these reasons we decided to use only the Tatoeba sentence pairs\n",
    "\n",
    "The Tatoeba site's translations are organized in a round-about way. There are three files needed: English sentences,\n",
    "Hebrew sentences, and links. Each sentense has a unique ID. The links file contains over 18 million ID pairs that\n",
    "represents those sentences are translations of themselves.\n",
    "\n",
    "In order to get just the English and Hebrew sentences we first looped through all 18 million ID pairs and looked \n",
    "within the English sentence Data Frame (df) and Hebrew sentence df for the presence of the current IDs [1]. We used a\n",
    "threading library because the processing of each row takes ~.002 seconds; with 18 million rows, the processing takes\n",
    "around 10hrs [2]. We left the process running overnight.\n",
    "If both IDs are found, the IDs are put into a holder df and then saved after every 100000 link pairs [3].\n",
    "Next we concatinated all df chunks together into a single df and file [4].\n",
    "Finally we built another df and file that contained the English/Hebrew texts instead of their respective IDs [5].\n",
    "Additionally, we stripped the sentences of punctuations (.?,!:;) because while they are used the same in Hebrew,\n",
    "we felt it would be easier to translate without them [6].\n",
    "(Numbers in brackets are to show where which step is within the code.)\n",
    "\n",
    "Ultimately we have 126,549 English-Hebrew sentence pairs. In the future we may decide to tokenize each sentence and \n",
    "build vocabularies; but for the time being the sentence pairs will remain together.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk, chunk_num):\n",
    "    added_count = 0\n",
    "    links = pd.DataFrame(columns = ['SENTENCE_ID','TRANSLATION_ID'])\n",
    "    for i,l in chunk.iterrows():\n",
    "        print(f'chunk {chunk_num} index {i} added {added_count}')\n",
    "        l1 = eng.loc[eng['SENTENCE_ID'] == l['SENTENCE_ID']]#[1]\n",
    "        l2 = heb.loc[heb['SENTENCE_ID'] == l['TRANSLATION_ID']]\n",
    "        if not l1.empty and not l2.empty:#[3]\n",
    "            id1 = l1.iloc[0]['SENTENCE_ID']\n",
    "            id2 = l2.iloc[0]['SENTENCE_ID']\n",
    "            links=links.append(pd.DataFrame({'SENTENCE_ID':[id1],'TRANSLATION_ID':[id2]}),ignore_index=True)\n",
    "            #print(len(links.index))\n",
    "            added_count+=1\n",
    "    links.to_csv(f'needed_links_{chunk_num}.csv',sep='\\t')\n",
    "    \n",
    "\n",
    "eng = pd.read_csv(\"eng_sentences.tsv\", sep=\"\\t\", names=[\"SENTENCE_ID\",\"LANGUAGE\",\"TEXT\"])\n",
    "heb = pd.read_csv(\"heb_sentences.tsv\", sep=\"\\t\", names=[\"SENTENCE_ID\",\"LANGUAGE\",\"TEXT\"])\n",
    "link_chunks = pd.read_csv(\"links.csv\", sep=\"\\t\", names=[\"SENTENCE_ID\",\"TRANSLATION_ID\"], chunksize=100000)\n",
    "for i, chunk in enumerate(link_chunks):\n",
    "    threading.Thread(target=process_chunk,args=(chunk,i)).start()#[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_links = pd.DataFrame(columns=['SENTENCE_ID','TRANSLATION_ID'])\n",
    "for i in range(183):#[4]\n",
    "    print(f'file number {i} total links {len(full_links.index)}')\n",
    "    chunk = pd.read_csv(f'needed_links_{i}.csv',sep='\\t')\n",
    "    for j,l in chunk.iterrows():\n",
    "        #print(l)\n",
    "        full_links=full_links.append(pd.DataFrame({'SENTENCE_ID':[l['SENTENCEE_ID']],'TRANSLATION_ID':[l['TRANSLATION_ID']]}),ignore_index=True)\n",
    "full_links.to_csv(f'needed_links_full.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "translations = pd.DataFrame(columns=['ENGLISH','HEBREW'])\n",
    "for i,l in full_links.iterrows():#[5]\n",
    "    if i%1000==0:\n",
    "        print(f'index {i}')\n",
    "    l1 = eng.loc[eng['SENTENCE_ID'] == l['SENTENCE_ID']]\n",
    "    l2 = heb.loc[heb['SENTENCE_ID'] == l['TRANSLATION_ID']]\n",
    "    if not l1.empty and not l2.empty:\n",
    "        id1 = l1.iloc[0]['SENTENCE_ID']\n",
    "        id2 = l2.iloc[0]['SENTENCE_ID']\n",
    "        if not id1 in ids and not id2 in ids:\n",
    "            ids.append(id1)\n",
    "            ids.append(id2)\n",
    "            t1 = ''.join(p for p in l1.iloc[0]['TEXT'] if p not in ['.','?',',','!',':',';'])#[6]\n",
    "            t2 = ''.join(p for p in l2.iloc[0]['TEXT'] if p not in ['.','?',',','!',':',';'])\n",
    "            translations = translations.append(pd.DataFrame({'ENGLISH':[t1],'HEBREW':[t2]}),ignore_index=True)\n",
    "translations.to_csv('translations.csv',sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
